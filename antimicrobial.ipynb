{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify antimicrobial peptides\n",
    "#Data and paper https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-019-3327-y\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras_self_attention import SeqSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np \n",
    "\n",
    "positives_train=[]\n",
    "negatives_train=[]\n",
    "positives_test=[]\n",
    "negatives_test=[]\n",
    "\n",
    "for seq_record in SeqIO.parse('AMP.tr.fa', \"fasta\"):\n",
    "    \n",
    "    positives_train.append(str(seq_record.seq))\n",
    "    \n",
    "for seq_record in SeqIO.parse('DECOY.tr.fa', \"fasta\"):\n",
    "    \n",
    "    negatives_train.append(str(seq_record.seq))\n",
    "    \n",
    "for seq_record in SeqIO.parse('DECOY.te.fa', \"fasta\"):\n",
    "    \n",
    "    negatives_test.append(str(seq_record.seq)) \n",
    "    \n",
    "for seq_record in SeqIO.parse('AMP.te.fa', \"fasta\"):\n",
    "    \n",
    "    positives_test.append(str(seq_record.seq))\n",
    "    \n",
    "    \n",
    "X_train=list(positives_train+negatives_train)\n",
    "X_train=np.array(X_train)\n",
    "\n",
    "y_train=list(np.ones(np.array(positives_train).shape[0]))+list(np.zeros(np.array(negatives_train).shape[0]))\n",
    "y_train=np.array(y_train)\n",
    "\n",
    "X_test=list(positives_test+negatives_test)\n",
    "X_test=np.array(X_train)\n",
    "\n",
    "y_test=list(np.ones(np.array(positives_test).shape[0]))+list(np.zeros(np.array(negatives_test).shape[0]))\n",
    "y_test=np.array(y_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "def string_to_array(my_string):\n",
    "    my_string = my_string.lower()\n",
    "    my_string = re.sub('[^arndcqeghilkmfpstwyvx]', 'z', my_string)\n",
    "    my_array = np.array(list(my_string))\n",
    "    return my_array\n",
    "\n",
    "# create a label encoder with alphabet\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(np.array(['a','r','n','d','c','q','e','g','h','i','l','k','m','f','p','s','t','w','y','v','x','z']))\n",
    "\n",
    "def ordinal_encoder(my_array):\n",
    "    integer_encoded = label_encoder.transform(my_array)\n",
    "    float_encoded = integer_encoded.astype(float)\n",
    "    float_encoded[float_encoded == 0] = 1.25 # A\n",
    "    float_encoded[float_encoded == 1] = 2.50 # R\n",
    "    float_encoded[float_encoded == 2] = 3.75 # N\n",
    "    float_encoded[float_encoded == 3] = 4.30 # D\n",
    "    float_encoded[float_encoded == 4] = 5.45 # C\n",
    "    float_encoded[float_encoded == 5] = 6.35 # Q\n",
    "    float_encoded[float_encoded == 6] = 7.85 # E \n",
    "    float_encoded[float_encoded == 7] = 8.65 # G\n",
    "    float_encoded[float_encoded == 8] = 9.95 # H\n",
    "    float_encoded[float_encoded == 9] = 11.25 # I\n",
    "    float_encoded[float_encoded == 10] = 12.55 # L\n",
    "    float_encoded[float_encoded == 11] = 13.15 # K\n",
    "    float_encoded[float_encoded == 12] = 14.11 # L\n",
    "    float_encoded[float_encoded == 13] = 15.29 # K\n",
    "    float_encoded[float_encoded == 14] = 16.39 # M\n",
    "    float_encoded[float_encoded == 15] = 17.05 # F\n",
    "    float_encoded[float_encoded == 16] = 18.09 # P\n",
    "    float_encoded[float_encoded == 17] = 19.49 # S\n",
    "    float_encoded[float_encoded == 18] = 20.79 # T\n",
    "    float_encoded[float_encoded == 19] = 21.19 # W\n",
    "    float_encoded[float_encoded == 20] = 22.55 # Y\n",
    "    float_encoded[float_encoded == 21] = 23.95 # V\n",
    "    float_encoded[float_encoded == 22] = 24.45 # X\n",
    "    float_encoded[float_encoded == 10] = 0.0 # anything else z\n",
    "    \n",
    "\n",
    "    return float_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X=list(X_train)\n",
    "for i in X_test : \n",
    "    X.append(i) \n",
    "\n",
    "X=[np.array(ordinal_encoder(string_to_array(i))) for i in X]\n",
    "X=np.array(X)\n",
    "X=pad_sequences(X)\n",
    "\n",
    "X=X.reshape(X.shape[0],X.shape[1],1)\n",
    "\n",
    "X_train=X[0:X_train.shape[0]]\n",
    "X_test=X[X_train.shape[0]:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train2 = keras.utils.to_categorical(y_train)\n",
    "y_test2 = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "# import necessary building blocks\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D,  Flatten, Dense, Activation, Dropout,BatchNormalization,LSTM, MaxPool1D\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    \"\"\"\n",
    "    Define your model architecture here.\n",
    "    Returns `Sequential` model.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv1D(input_shape=X_train[0].shape,padding=\"same\",kernel_size=3,filters=16))\n",
    "    model.add(LSTM(100, return_sequences=True))\n",
    "    model.add(MaxPool1D())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=32))\n",
    "    model.add(MaxPool1D())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv1D(padding=\"same\",kernel_size=3,filters=64))\n",
    "    model.add(MaxPool1D())\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    model.add(Dense(256))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(LeakyReLU(0.1))\n",
    "    \n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_48 (Conv1D)           (None, 158, 16)           64        \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 158, 100)          46800     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 79, 100)           0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_63 (LeakyReLU)   (None, 79, 100)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 79, 100)           400       \n",
      "_________________________________________________________________\n",
      "conv1d_49 (Conv1D)           (None, 79, 32)            9632      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)   (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 79, 32)            128       \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 79, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 79, 32)            3104      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 39, 32)            0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 39, 32)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 39, 32)            128       \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 39, 64)            6208      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 19, 64)            256       \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 19, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1216)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               311552    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 2)                 514       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 378,786\n",
      "Trainable params: 378,330\n",
      "Non-trainable params: 456\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "45/45 [==============================] - 9s 204ms/step - loss: 0.7478 - accuracy: 0.5084 - val_loss: 0.6927 - val_accuracy: 0.5169\n",
      "Epoch 2/50\n",
      "45/45 [==============================] - 8s 175ms/step - loss: 0.7084 - accuracy: 0.5253 - val_loss: 0.6924 - val_accuracy: 0.5162\n",
      "Epoch 3/50\n",
      "45/45 [==============================] - 8s 174ms/step - loss: 0.6925 - accuracy: 0.5162 - val_loss: 0.6945 - val_accuracy: 0.5000\n",
      "Epoch 4/50\n",
      "45/45 [==============================] - 8s 172ms/step - loss: 0.6819 - accuracy: 0.5702 - val_loss: 0.6936 - val_accuracy: 0.5000\n",
      "Epoch 5/50\n",
      "45/45 [==============================] - 8s 175ms/step - loss: 0.6896 - accuracy: 0.5808 - val_loss: 0.6885 - val_accuracy: 0.5211\n",
      "Epoch 6/50\n",
      "45/45 [==============================] - 8s 172ms/step - loss: 0.6739 - accuracy: 0.5850 - val_loss: 0.6834 - val_accuracy: 0.5513\n",
      "Epoch 7/50\n",
      "45/45 [==============================] - 8s 171ms/step - loss: 0.6592 - accuracy: 0.6194 - val_loss: 0.6825 - val_accuracy: 0.5463\n",
      "Epoch 8/50\n",
      "45/45 [==============================] - 8s 172ms/step - loss: 0.6557 - accuracy: 0.6222 - val_loss: 0.6645 - val_accuracy: 0.6018\n",
      "Epoch 9/50\n",
      "45/45 [==============================] - 8s 174ms/step - loss: 0.6389 - accuracy: 0.6390 - val_loss: 0.6479 - val_accuracy: 0.6187\n",
      "Epoch 10/50\n",
      "45/45 [==============================] - 8s 173ms/step - loss: 0.6251 - accuracy: 0.6573 - val_loss: 0.6400 - val_accuracy: 0.6243\n",
      "Epoch 11/50\n",
      "45/45 [==============================] - 8s 175ms/step - loss: 0.6157 - accuracy: 0.6777 - val_loss: 0.6357 - val_accuracy: 0.6025\n",
      "Epoch 12/50\n",
      "45/45 [==============================] - 8s 174ms/step - loss: 0.6190 - accuracy: 0.6770 - val_loss: 0.6157 - val_accuracy: 0.6777\n",
      "Epoch 13/50\n",
      "45/45 [==============================] - 8s 175ms/step - loss: 0.6062 - accuracy: 0.6713 - val_loss: 0.6044 - val_accuracy: 0.6882\n",
      "Epoch 14/50\n",
      "45/45 [==============================] - 9s 192ms/step - loss: 0.5919 - accuracy: 0.6819 - val_loss: 0.5697 - val_accuracy: 0.7268\n",
      "Epoch 15/50\n",
      "45/45 [==============================] - 9s 203ms/step - loss: 0.6063 - accuracy: 0.6763 - val_loss: 0.6532 - val_accuracy: 0.6201\n",
      "Epoch 16/50\n",
      "45/45 [==============================] - 8s 184ms/step - loss: 0.5871 - accuracy: 0.6903 - val_loss: 0.5942 - val_accuracy: 0.6657\n",
      "Epoch 17/50\n",
      "45/45 [==============================] - 8s 183ms/step - loss: 0.5769 - accuracy: 0.7128 - val_loss: 0.5428 - val_accuracy: 0.7135\n",
      "Epoch 18/50\n",
      "45/45 [==============================] - 8s 175ms/step - loss: 0.5541 - accuracy: 0.7107 - val_loss: 0.5183 - val_accuracy: 0.7612\n",
      "Epoch 19/50\n",
      "45/45 [==============================] - 8s 177ms/step - loss: 0.5581 - accuracy: 0.7135 - val_loss: 0.5890 - val_accuracy: 0.6770\n",
      "Epoch 20/50\n",
      "45/45 [==============================] - 8s 174ms/step - loss: 0.5482 - accuracy: 0.7324 - val_loss: 0.5223 - val_accuracy: 0.7444\n",
      "Epoch 21/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.5451 - accuracy: 0.7205 - val_loss: 0.5708 - val_accuracy: 0.6973\n",
      "Epoch 22/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.5295 - accuracy: 0.7458 - val_loss: 0.6127 - val_accuracy: 0.6475\n",
      "Epoch 23/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.5217 - accuracy: 0.7409 - val_loss: 0.4760 - val_accuracy: 0.7739\n",
      "Epoch 24/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.5311 - accuracy: 0.7303 - val_loss: 0.5735 - val_accuracy: 0.6784\n",
      "Epoch 25/50\n",
      "45/45 [==============================] - 8s 184ms/step - loss: 0.5026 - accuracy: 0.7626 - val_loss: 0.5233 - val_accuracy: 0.7107\n",
      "Epoch 26/50\n",
      "45/45 [==============================] - 8s 176ms/step - loss: 0.4906 - accuracy: 0.7704 - val_loss: 0.4253 - val_accuracy: 0.8034\n",
      "Epoch 27/50\n",
      "45/45 [==============================] - 8s 180ms/step - loss: 0.4764 - accuracy: 0.7774 - val_loss: 0.4218 - val_accuracy: 0.8048\n",
      "Epoch 28/50\n",
      "45/45 [==============================] - 8s 180ms/step - loss: 0.4798 - accuracy: 0.7858 - val_loss: 0.4579 - val_accuracy: 0.7711\n",
      "Epoch 29/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.4596 - accuracy: 0.7795 - val_loss: 0.4356 - val_accuracy: 0.7971\n",
      "Epoch 30/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.4433 - accuracy: 0.7978 - val_loss: 0.4151 - val_accuracy: 0.8069\n",
      "Epoch 31/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.4364 - accuracy: 0.8020 - val_loss: 0.5347 - val_accuracy: 0.7240\n",
      "Epoch 32/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.4254 - accuracy: 0.8125 - val_loss: 0.3695 - val_accuracy: 0.8399\n",
      "Epoch 33/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.4213 - accuracy: 0.8020 - val_loss: 0.3605 - val_accuracy: 0.8399\n",
      "Epoch 34/50\n",
      "45/45 [==============================] - 8s 177ms/step - loss: 0.3832 - accuracy: 0.8265 - val_loss: 0.3037 - val_accuracy: 0.8771\n",
      "Epoch 35/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.3698 - accuracy: 0.8294 - val_loss: 0.2938 - val_accuracy: 0.8834\n",
      "Epoch 36/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.3517 - accuracy: 0.8420 - val_loss: 0.3136 - val_accuracy: 0.8827\n",
      "Epoch 37/50\n",
      "45/45 [==============================] - 8s 177ms/step - loss: 0.3555 - accuracy: 0.8287 - val_loss: 0.6608 - val_accuracy: 0.6910\n",
      "Epoch 38/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.3592 - accuracy: 0.8287 - val_loss: 0.3053 - val_accuracy: 0.8588\n",
      "Epoch 39/50\n",
      "45/45 [==============================] - 8s 182ms/step - loss: 0.3240 - accuracy: 0.8610 - val_loss: 0.2917 - val_accuracy: 0.8778\n",
      "Epoch 40/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.3108 - accuracy: 0.8659 - val_loss: 0.2951 - val_accuracy: 0.8603\n",
      "Epoch 41/50\n",
      "45/45 [==============================] - 8s 178ms/step - loss: 0.3159 - accuracy: 0.8610 - val_loss: 0.3586 - val_accuracy: 0.8315\n",
      "Epoch 42/50\n",
      "45/45 [==============================] - 8s 186ms/step - loss: 0.3197 - accuracy: 0.8476 - val_loss: 0.1996 - val_accuracy: 0.9382\n",
      "Epoch 43/50\n",
      "45/45 [==============================] - 8s 183ms/step - loss: 0.2761 - accuracy: 0.8897 - val_loss: 0.3709 - val_accuracy: 0.8258\n",
      "Epoch 44/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.2956 - accuracy: 0.8694 - val_loss: 0.1780 - val_accuracy: 0.9480\n",
      "Epoch 45/50\n",
      "45/45 [==============================] - 8s 181ms/step - loss: 0.2687 - accuracy: 0.8855 - val_loss: 0.6866 - val_accuracy: 0.7353\n",
      "Epoch 46/50\n",
      "45/45 [==============================] - 8s 181ms/step - loss: 0.2747 - accuracy: 0.8813 - val_loss: 0.3605 - val_accuracy: 0.8181\n",
      "Epoch 47/50\n",
      "45/45 [==============================] - 8s 177ms/step - loss: 0.2360 - accuracy: 0.9010 - val_loss: 0.1872 - val_accuracy: 0.9185\n",
      "Epoch 48/50\n",
      "45/45 [==============================] - 8s 181ms/step - loss: 0.2396 - accuracy: 0.8912 - val_loss: 0.2459 - val_accuracy: 0.8869\n",
      "Epoch 49/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.2629 - accuracy: 0.8947 - val_loss: 0.2323 - val_accuracy: 0.9101\n",
      "Epoch 50/50\n",
      "45/45 [==============================] - 8s 179ms/step - loss: 0.2368 - accuracy: 0.8919 - val_loss: 0.1411 - val_accuracy: 0.9522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1402f9370>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "INIT_LR = 5e-3  # initial learning rate\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "\n",
    "\n",
    "# don't call K.set_learning_phase() !!! (otherwise will enable dropout in train/test simultaneously)\n",
    "model = make_model()  # define our model\n",
    "\n",
    "# prepare model for fitting (loss, optimizer, etc)\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',  # we train 2-way classification\n",
    "    optimizer=keras.optimizers.Adamax(lr=INIT_LR),  # for SGD\n",
    "    metrics=['accuracy']  # report accuracy during training\n",
    ")\n",
    "\n",
    "# fit model\n",
    "model.fit(\n",
    "    X_train, y_train2,  # prepared data\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    \n",
    "    validation_data=(X_test, y_test2),\n",
    "    shuffle=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test).argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_answers = y_test2.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = np.mean(test_predictions==test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.2247191011236%\n"
     ]
    }
   ],
   "source": [
    "print(str(test_accuracy*100)+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Design new ones using an LSTM\n",
    "#Using code from https://github.com/tadeaspaule/universal-name-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Input, concatenate, Reshape, Dropout, Bidirectional\n",
    "from tensorflow.keras.models import Model, load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np \n",
    "\n",
    "positives_train=[]\n",
    "negatives_train=[]\n",
    "positives_test=[]\n",
    "negatives_test=[]\n",
    "\n",
    "for seq_record in SeqIO.parse('AMP.tr.fa', \"fasta\"):\n",
    "    \n",
    "    positives_train.append(str(seq_record.seq))\n",
    "    \n",
    "for seq_record in SeqIO.parse('DECOY.tr.fa', \"fasta\"):\n",
    "    \n",
    "    negatives_train.append(str(seq_record.seq))\n",
    "    \n",
    "for seq_record in SeqIO.parse('DECOY.te.fa', \"fasta\"):\n",
    "    \n",
    "    negatives_test.append(str(seq_record.seq)) \n",
    "    \n",
    "for seq_record in SeqIO.parse('AMP.te.fa', \"fasta\"):\n",
    "    \n",
    "    positives_test.append(str(seq_record.seq))\n",
    "    \n",
    "    \n",
    "X_train=list(positives_train+negatives_train)\n",
    "X_train=np.array(X_train)\n",
    "\n",
    "y_train=list(np.ones(np.array(positives_train).shape[0]))+list(np.zeros(np.array(negatives_train).shape[0]))\n",
    "y_train=np.array(y_train)\n",
    "\n",
    "X_test=list(positives_test+negatives_test)\n",
    "X_test=np.array(X_train)\n",
    "\n",
    "\n",
    "\n",
    "X=list(X_train)\n",
    "for i in list(X_test) : \n",
    "    X.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 2848\n",
      "Amount of names after removing those with unwanted characters\n",
      ": 2848\n",
      "Using the following characters:\n",
      " ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n"
     ]
    }
   ],
   "source": [
    "def process_names(names,*,unwanted=[]):\n",
    "    names = [name for name in names]\n",
    "    print(\"Total names:\",len(names))\n",
    "    chars = sorted(list(set(''.join(names))))\n",
    "\n",
    "    def has_unwanted(word):\n",
    "        for char in word:\n",
    "            if char in unwanted:\n",
    "                return True\n",
    "        return False\n",
    "    names = [name for name in names if not has_unwanted(name)]\n",
    "    print(\"Amount of names after removing those with unwanted characters\\n:\",len(names))\n",
    "    chars = [char for char in chars if char not in unwanted]\n",
    "    print(\"Using the following characters:\\n\",chars)\n",
    "\n",
    "\n",
    "\n",
    "    return names,chars\n",
    "\n",
    "\n",
    "names,chars = process_names(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89504 sequences of length 4 made\n"
     ]
    }
   ],
   "source": [
    "def make_sequences(names,seqlen):\n",
    "    sequences, lengths, nextchars = [],[],[] # To have the model learn a more macro understanding, \n",
    "                                             # it also takes the word's length so far as input\n",
    "    for name in names:\n",
    "        if len(name) <= seqlen:\n",
    "            sequences.append(name + chars[-1]*(seqlen - len(name)))\n",
    "            nextchars.append(chars[-1])\n",
    "            lengths.append(len(name))\n",
    "        else:\n",
    "            for i in range(0,len(name)-seqlen+1):\n",
    "                sequences.append(name[i:i+seqlen])\n",
    "                if i+seqlen < len(name):\n",
    "                    nextchars.append(name[i+seqlen])\n",
    "                else:\n",
    "                    nextchars.append(chars[-1])\n",
    "                lengths.append(i+seqlen)\n",
    "\n",
    "    print(len(sequences),\"sequences of length\",seqlen,\"made\")\n",
    "    \n",
    "    return sequences,lengths,nextchars\n",
    "\n",
    "seqlen = 4\n",
    "sequences,lengths,nextchars = make_sequences(names,seqlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_onehots(*,sequences,lengths,nextchars,chars):\n",
    "    x = np.zeros(shape=(len(sequences),len(sequences[0]),len(chars)), dtype='float32') # sequences\n",
    "    x2 = np.zeros(shape=(len(lengths),max(lengths))) # lengths\n",
    "\n",
    "    for i, seq in enumerate(sequences):\n",
    "        for j, char in enumerate(seq):\n",
    "            x[i,j,chars.index(char)] = 1.\n",
    "\n",
    "    for i, l in enumerate(lengths):\n",
    "        x2[i,l-1] = 1.\n",
    "\n",
    "    y = np.zeros(shape=(len(nextchars),len(chars)))\n",
    "    for i, char in enumerate(nextchars):\n",
    "        y[i,chars.index(char)] = 1.\n",
    "    \n",
    "    return x,x2,y\n",
    "\n",
    "x,x2,y = make_onehots(sequences=sequences,\n",
    "                     lengths=lengths,\n",
    "                     nextchars=nextchars,\n",
    "                     chars=chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dictchars(names,seqlen):\n",
    "    dictchars = [{} for _ in range(seqlen)]\n",
    "\n",
    "    for name in names:\n",
    "        if len(name) < seqlen:\n",
    "            continue\n",
    "        dictchars[0][name[0]] = dictchars[0].get(name[0],0) + 1\n",
    "        for i in range(1,seqlen):\n",
    "            if dictchars[i].get(name[i-1],0) == 0:\n",
    "                dictchars[i][name[i-1]] = {name[i]: 1}\n",
    "            elif dictchars[i][name[i-1]].get(name[i],0) == 0:\n",
    "                dictchars[i][name[i-1]][name[i]] = 1\n",
    "            else:\n",
    "                dictchars[i][name[i-1]][name[i]] += 1\n",
    "    return dictchars\n",
    "                \n",
    "dictchars = get_dictchars(names,seqlen)\n",
    "                \n",
    "\n",
    "def generate_start_seq(dictchars):\n",
    "    res = \"\" # The starting sequence will be stored here\n",
    "    p = sum([n for n in dictchars[0].values()]) # total amount of letter occurences\n",
    "    r = np.random.randint(0,p) # random number used to pick the next character\n",
    "    tot = 0\n",
    "    for key, item in dictchars[0].items():\n",
    "        if r >= tot and r < tot + item:\n",
    "            res += key\n",
    "            break\n",
    "        else:\n",
    "            tot += item\n",
    "\n",
    "    for i in range(1,len(dictchars)):\n",
    "        ch = res[-1]\n",
    "        if dictchars[i].get(ch,0) == 0:\n",
    "            l = list(dictchars[i].keys())\n",
    "            ch = l[np.random.randint(0,len(l))]\n",
    "        p = sum([n for n in dictchars[i][ch].values()])\n",
    "        r = np.random.randint(0,p)\n",
    "        tot = 0\n",
    "        for key, item in dictchars[i][ch].items():\n",
    "            if r >= tot and r < tot + item:\n",
    "                res += key\n",
    "                break\n",
    "            else:\n",
    "                tot += item\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds,temperature=0.4):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    if temperature == 0:\n",
    "        # Avoiding a division by 0 error\n",
    "        return np.argmax(preds)\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1,preds,1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_name(model,start,*,chars=chars,temperature=0.4):\n",
    "    maxlength = model.layers[3].input.shape[1]\n",
    "    seqlen = int(model.layers[0].input.shape[1])\n",
    "    result = start\n",
    "    \n",
    "    sequence_input = np.zeros(shape=(1,seqlen,len(chars)))\n",
    "    for i, char in enumerate(start):\n",
    "        sequence_input[0,i,chars.index(char)] = 1.\n",
    "    \n",
    "    length_input = np.zeros(shape=(1,maxlength))\n",
    "    length_input[0,len(result)-1] = 1.\n",
    "    \n",
    "    prediction = model.predict(x=[sequence_input,length_input])[0]\n",
    "    char_index = sample(prediction,temperature)\n",
    "    while char_index < len(chars)-1 and len(result) < maxlength:\n",
    "        result += chars[char_index]\n",
    "        \n",
    "        sequence_input = np.zeros(shape=(1,seqlen,len(chars)))\n",
    "        for i, char in enumerate(result[(-seqlen):]):\n",
    "            sequence_input[0,i,chars.index(char)] = 1.\n",
    "        \n",
    "        length_input[0,len(result)-2] = 0.\n",
    "        length_input[0,len(result)-1] = 1.\n",
    "        \n",
    "        prediction = model.predict(x=[sequence_input,length_input])[0]\n",
    "        char_index = sample(prediction,temperature)\n",
    "    \n",
    "    return result.title()\n",
    "\n",
    "def generate_random_name(model,*,chars=chars,dictchars=dictchars,temperature=0.4):\n",
    "    start = generate_start_seq(dictchars)\n",
    "    return generate_name(model,start,chars=chars,temperature=temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_model(x,x2,chars):\n",
    "    inp1 = Input(shape=x.shape[1:]) # sequence input\n",
    "    inp2 = Input(shape=x2.shape[1:]) # length input\n",
    "    lstm = Bidirectional(LSTM(len(chars),activation='relu',dropout=0.3))(inp1)\n",
    "    lstm2 = Bidirectional(LSTM(len(chars),dropout=0.3,go_backwards=True))(inp1)\n",
    "    concat = concatenate([lstm,lstm2,inp2])\n",
    "    dense = Dense(len(chars),activation='softmax')(concat)\n",
    "\n",
    "    model = Model([inp1,inp2],dense)\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "    return model\n",
    "\n",
    "model = make_model(x,x2,chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying only for 3 epochs. Increase number of epochs for better results and more sequences.\n",
    "generated=[]\n",
    "\n",
    "def try_model(model,*,x=x,x2=x2,y=y,chars=chars,dictchars=dictchars,total_epochs=3,print_every=1,temperature=0.4,verbose=True):\n",
    "    for i in range(total_epochs//print_every):\n",
    "        history = model.fit([x,x2],y,\n",
    "                            epochs=print_every,\n",
    "                            batch_size=64,\n",
    "                            validation_split=0.05,\n",
    "                            verbose=0)\n",
    "\n",
    "        generated.append((generate_random_name(model,chars=chars,dictchars=dictchars,temperature=temperature)).upper())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EIMDLKLAIAGIKVCTGVKIDKEF',\n",
       " 'VVSLLAKKIGKSLKGKIAGIGAAAAAAGHGGACGGAGGVHGGAG',\n",
       " 'ILLCADCKLLKKLAKKIRKK']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
